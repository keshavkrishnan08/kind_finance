{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# KTND-Finance: Full Experiment Pipeline (v1.5.1)\n\n### How to run (FRESH):\n1. **Cell 1** (Setup) -- Install deps + clone repo (~2 min)\n2. **Cell 2** (Run Everything) -- Full pipeline + multi-seed + ablations + gyrator (~7-10 hours)\n3. **Cell 3** (View) -- Display all figures\n4. **Cell 4** (Download) -- Zip all results\n\n### How to run (AFTER Cell 2 already completed Part A):\n1. **Cell 1** (Setup) -- pull latest code\n2. **Cell 2b** (Minimal re-run) -- KTND HMM fix + resume ablations + gyrator (~6 hours)\n3. **Cell 3** (View) -- Display all figures\n4. **Cell 4** (Download)\n\n### What changed (v1.5.1):\n- **HMM regime detection** -- replaced eigenfunction sign thresholding with 2-state Gaussian HMM on top 3 eigenfunctions (uses temporal transition structure + amplitude info)\n- **Multi-seed error bars** -- 5 seeds for main results\n- **GARCH(1,1) baseline** -- standard econophysics volatility regime detector\n- **Granger date alignment fix** -- spectral gap vs VIX now date-indexed\n- **Model improvements** -- beta_orth 0.01, n_modes 5 (univariate), permutation n_segments 20\n\nSet runtime to **GPU (T4)**: Runtime -> Change runtime type -> T4 GPU"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup (install + clone + verify) - ~2 min\n",
    "\n",
    "# Install missing dependencies (torch/numpy/pandas/scipy/sklearn/matplotlib are pre-installed)\n",
    "!pip install -q yfinance>=1.0.0 hmmlearn>=0.3.0 statsmodels>=0.14.0 arch>=6.0.0 pyyaml>=6.0\n",
    "\n",
    "# Clone repo\n",
    "import os, sys\n",
    "REPO_URL = \"https://github.com/keshavkrishnan08/kind_finance.git\"\n",
    "REPO_DIR = \"/content/ktnd_finance\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !cd {REPO_DIR} && git pull\n",
    "else:\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Verify\n",
    "import torch, numpy as np\n",
    "from src.model.vampnet import NonEquilibriumVAMPNet\n",
    "print(f\"Python {sys.version.split()[0]} | PyTorch {torch.__version__} | \"\n",
    "      f\"CUDA: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2. Run EVERYTHING â€” pipeline + ablations + gyrator (~5-7 hours, walk away)\n\nimport subprocess, time, json, os, sys, glob\n\n# ==========================================================================\n# PATHS\n# ==========================================================================\nREPO_DIR = \"/content/ktnd_finance\"\nOUTPUT_DIR = \"/content/ktnd_finance/outputs\"\nRESULTS_DIR = \"/content/ktnd_finance/outputs/results\"\nMODELS_DIR = \"/content/ktnd_finance/outputs/models\"\nFIGURES_DIR = \"/content/ktnd_finance/outputs/figures\"\nDATA_DIR = \"/content/ktnd_finance/data\"\nN_ABLATION_SEEDS = 10  # PRE requires >=10 for reliable std errors\n\nfor d in [OUTPUT_DIR, RESULTS_DIR, MODELS_DIR, FIGURES_DIR]:\n    os.makedirs(d, exist_ok=True)\n\nos.chdir(REPO_DIR)\npython = sys.executable\n\nprint(f\"Python: {python}\")\nprint(f\"CWD: {os.getcwd()}\")\nprint(f\"Repo dir exists: {os.path.exists(REPO_DIR)}\")\nprint(f\"src/ exists: {os.path.isdir(os.path.join(REPO_DIR, 'src'))}\")\nprint(f\"experiments/ exists: {os.path.isdir(os.path.join(REPO_DIR, 'experiments'))}\")\nprint(f\"config/ exists: {os.path.isdir(os.path.join(REPO_DIR, 'config'))}\")\n\ndef run(name, cmd, check_files=None):\n    \"\"\"Run a stage, print output, verify files.\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"  STAGE: {name}\")\n    print(f\"  CMD: {cmd}\")\n    print(f\"{'='*70}\")\n    t0 = time.time()\n    result = subprocess.run(cmd, shell=True, cwd=REPO_DIR,\n                            capture_output=True, text=True)\n    elapsed = time.time() - t0\n\n    if result.stdout:\n        for line in result.stdout.strip().split('\\n'):\n            print(f\"  {line}\")\n\n    if result.returncode != 0:\n        print(f\"\\n  === STDERR ===\")\n        if result.stderr:\n            for line in result.stderr.strip().split('\\n'):\n                print(f\"  ! {line}\")\n        print(f\"  >> {name}: FAILED (exit code {result.returncode}, {elapsed/60:.1f} min)\")\n        return False\n\n    if result.stderr:\n        stderr_lines = result.stderr.strip().split('\\n')\n        error_lines = [l for l in stderr_lines if 'Error' in l or 'Exception' in l or 'Traceback' in l]\n        if error_lines:\n            print(f\"  === STDERR (errors) ===\")\n            for line in error_lines:\n                print(f\"  ! {line}\")\n\n    if check_files:\n        missing = [f for f in check_files if not os.path.exists(f)]\n        if missing:\n            print(f\"  WARNING: Missing expected output files:\")\n            for f in missing:\n                print(f\"    MISSING: {f}\")\n            if result.stderr:\n                for line in result.stderr.strip().split('\\n')[-30:]:\n                    print(f\"  ! {line}\")\n            print(f\"  >> {name}: INCOMPLETE ({elapsed/60:.1f} min)\")\n            return False\n        for f in check_files:\n            sz = os.path.getsize(f)\n            print(f\"  OK: {os.path.basename(f)} ({sz:,} bytes)\")\n\n    print(f\"  >> {name}: OK ({elapsed/60:.1f} min)\")\n    return True\n\ndef run_streaming(name, cmd, check_files=None):\n    \"\"\"Run a stage with LIVE output streaming (for long-running tasks).\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"  STAGE: {name}\")\n    print(f\"  CMD: {cmd}\")\n    print(f\"{'='*70}\", flush=True)\n    t0 = time.time()\n    proc = subprocess.Popen(\n        cmd, shell=True, cwd=REPO_DIR,\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n        text=True, bufsize=1,\n    )\n    for line in proc.stdout:\n        print(f\"  {line}\", end=\"\", flush=True)\n    proc.wait()\n    elapsed = time.time() - t0\n\n    if proc.returncode != 0:\n        print(f\"  >> {name}: FAILED (exit code {proc.returncode}, {elapsed/60:.1f} min)\")\n        return False\n\n    if check_files:\n        missing = [f for f in check_files if not os.path.exists(f)]\n        if missing:\n            print(f\"  WARNING: Missing expected output files:\")\n            for f in missing:\n                print(f\"    MISSING: {f}\")\n            print(f\"  >> {name}: INCOMPLETE ({elapsed/60:.1f} min)\")\n            return False\n        for f in check_files:\n            sz = os.path.getsize(f)\n            print(f\"  OK: {os.path.basename(f)} ({sz:,} bytes)\")\n\n    print(f\"  >> {name}: OK ({elapsed/60:.1f} min)\")\n    return True\n\npipeline_start = time.time()\nresults = {}\n\n# ======================================================================\n# PART A: FULL PIPELINE (~2-3 hours)\n# ======================================================================\nprint(f\"\\n{'#'*70}\")\nprint(f\"#  PART A: FULL PIPELINE\")\nprint(f\"#  Order: tests -> download -> train(uni+multi) -> baselines ->\")\nprint(f\"#         rolling -> robustness(needs rolling output) -> figures\")\nprint(f\"{'#'*70}\")\n\n# --- Stage 1: Quick tests ---\nresults['tests'] = run('Quick tests',\n    f'{python} -m pytest tests/ -q --tb=short -k \"not test_synthetic\"')\n\n# --- Stage 2: Download data ---\nresults['download'] = run('Download data',\n    f'{python} {REPO_DIR}/data/download.py --mode all',\n    check_files=[f'{DATA_DIR}/prices.csv', f'{DATA_DIR}/vix.csv'])\n\n# --- Stage 3: Train univariate ---\n# NOTE: --config default.yaml ensures all loss weights/training params are loaded.\n# Mode-specific overrides (n_modes=5, hidden_dims, batch_size) merge from univariate.yaml.\nresults['train_uni'] = run('Train univariate (SPY)',\n    f'{python} {REPO_DIR}/experiments/run_main.py'\n    f' --config config/default.yaml --mode univariate --seed 42'\n    f' --output-dir {OUTPUT_DIR}',\n    check_files=[\n        f'{RESULTS_DIR}/analysis_results.json',\n        f'{RESULTS_DIR}/eigenvalues.csv',\n        f'{RESULTS_DIR}/entropy_decomposition.csv',\n        f'{RESULTS_DIR}/irreversibility_field.npy',\n        f'{MODELS_DIR}/vampnet_univariate.pt',\n    ])\n\n# --- Stage 4: Train multiasset ---\nresults['train_multi'] = run('Train multiasset (11 ETFs)',\n    f'{python} {REPO_DIR}/experiments/run_main.py'\n    f' --config config/default.yaml --mode multiasset --seed 42'\n    f' --output-dir {OUTPUT_DIR}',\n    check_files=[f'{RESULTS_DIR}/analysis_results_multiasset.json',\n                 f'{MODELS_DIR}/vampnet_multiasset.pt'])\n\n# --- Stage 5: Baselines ---\nresults['baselines'] = run('Baselines',\n    f'{python} {REPO_DIR}/experiments/run_baselines.py'\n    f' --config config/default.yaml --output-dir {RESULTS_DIR}',\n    check_files=[f'{RESULTS_DIR}/baseline_comparison.csv'])\n\n# --- Stage 6: Rolling (BEFORE robustness -- Granger needs spectral_gap_timeseries.csv) ---\nresults['rolling'] = run('Rolling spectral analysis',\n    f'{python} {REPO_DIR}/experiments/run_rolling.py'\n    f' --config config/default.yaml --mode univariate'\n    f' --checkpoint {MODELS_DIR}/vampnet_univariate.pt'\n    f' --output-dir {RESULTS_DIR}',\n    check_files=[f'{RESULTS_DIR}/spectral_gap_timeseries.csv'])\n\n# --- Stage 7: Robustness (AFTER rolling -- Granger needs spectral_gap_timeseries.csv) ---\nresults['robustness'] = run('Robustness tests',\n    f'{python} {REPO_DIR}/experiments/run_robustness.py'\n    f' --config config/default.yaml --mode univariate'\n    f' --checkpoint {MODELS_DIR}/vampnet_univariate.pt'\n    f' --output-dir {RESULTS_DIR}',\n    check_files=[f'{RESULTS_DIR}/statistical_tests.json'])\n\n# --- Stage 8: Figures (script) ---\nresults['figures'] = run('Generate figures (script)',\n    f'{python} {REPO_DIR}/experiments/run_figures.py'\n    f' --results-dir {RESULTS_DIR} --figures-dir {FIGURES_DIR}')\n\n# --- Stage 9: Figures (inline fallback) ---\nprint(f\"\\n{'='*70}\")\nprint(f\"  GENERATING FIGURES INLINE (FALLBACK)\")\nprint(f\"{'='*70}\")\n\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nn_figs = 0\n\nfor mode_tag, label in [(\"univariate\", \"Univariate (SPY)\"), (\"multiasset\", \"Multiasset\")]:\n    ap = f\"{RESULTS_DIR}/analysis_results_{mode_tag}.json\"\n    if not os.path.exists(ap):\n        continue\n    with open(ap) as f:\n        ar = json.load(f)\n\n    er, ei = ar.get(\"eigenvalues_real\"), ar.get(\"eigenvalues_imag\")\n    if er and ei:\n        er, ei = np.array(er), np.array(ei)\n        mags = np.sqrt(er**2 + ei**2)\n        fig, ax = plt.subplots(figsize=(7,7))\n        th = np.linspace(0, 2*np.pi, 300)\n        ax.plot(np.cos(th), np.sin(th), \"k--\", lw=0.8, alpha=0.5)\n        sc = ax.scatter(er, ei, c=mags, cmap=\"viridis\", edgecolors=\"k\", linewidths=0.4, s=80, zorder=3)\n        plt.colorbar(sc, ax=ax, label=\"|$\\\\lambda$|\")\n        for i, idx in enumerate(np.argsort(-mags)[:5]):\n            ax.annotate(f\"$\\\\lambda_{i}$\", (er[idx], ei[idx]), textcoords=\"offset points\", xytext=(8,8), fontsize=9)\n        ax.set_xlabel(\"Re($\\\\lambda$)\"); ax.set_ylabel(\"Im($\\\\lambda$)\")\n        ax.set_title(f\"Koopman Eigenvalue Spectrum -- {label}\"); ax.set_aspect(\"equal\"); ax.grid(True, alpha=0.3)\n        fig.savefig(f\"{FIGURES_DIR}/fig1_eigenvalue_spectrum_{mode_tag}.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\nfor csv_name, title, ycol in [\n    (\"eigenvalues.csv\", \"Eigenvalue Magnitudes\", \"magnitude\"),\n    (\"entropy_decomposition.csv\", \"Entropy Decomposition\", \"entropy_production\"),\n]:\n    p = f\"{RESULTS_DIR}/{csv_name}\"\n    if os.path.exists(p):\n        df = pd.read_csv(p)\n        if ycol in df.columns:\n            fig, ax = plt.subplots(figsize=(8,5))\n            ax.bar(df[\"mode\"], df[ycol], color=\"coral\" if \"entropy\" in csv_name else \"steelblue\", edgecolor=\"black\", lw=0.3)\n            ax.set_xlabel(\"Mode\"); ax.set_ylabel(ycol); ax.set_title(title); ax.grid(True, alpha=0.3, axis=\"y\")\n            fig.savefig(f\"{FIGURES_DIR}/fig_{csv_name.replace('.csv','')}.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\nirp = f\"{RESULTS_DIR}/irreversibility_field.npy\"\nif os.path.exists(irp):\n    ir = np.load(irp, allow_pickle=True)\n    fig, ax = plt.subplots(figsize=(14,4))\n    ax.fill_between(range(len(ir)), ir, alpha=0.4, color=\"darkorange\"); ax.plot(ir, lw=0.5, color=\"darkorange\")\n    ax.set_xlabel(\"Time\"); ax.set_ylabel(\"$I(x)$\"); ax.set_title(\"Irreversibility Field\"); ax.grid(True, alpha=0.3)\n    fig.savefig(f\"{FIGURES_DIR}/fig_irreversibility_field.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\nrcp = f\"{RESULTS_DIR}/spectral_gap_timeseries.csv\"\nif os.path.exists(rcp):\n    rdf = pd.read_csv(rcp)\n    if \"spectral_gap\" in rdf.columns:\n        fig, ax = plt.subplots(figsize=(14,5))\n        x = pd.to_datetime(rdf[\"center_date\"]) if \"center_date\" in rdf.columns else range(len(rdf))\n        ax.plot(x, rdf[\"spectral_gap\"], color=\"steelblue\", lw=1.0)\n        ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Spectral Gap\"); ax.set_title(\"Rolling Spectral Gap\"); ax.grid(True, alpha=0.3)\n        fig.savefig(f\"{FIGURES_DIR}/fig_spectral_gap.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\nbcp = f\"{RESULTS_DIR}/baseline_comparison.csv\"\nif os.path.exists(bcp):\n    bdf = pd.read_csv(bcp)\n    ms = [m for m in [\"nber_accuracy\",\"nber_f1\",\"nber_precision\",\"nber_recall\"] if m in bdf.columns]\n    if ms and \"method\" in bdf.columns:\n        fig, ax = plt.subplots(figsize=(10,6))\n        x = np.arange(len(bdf)); w = 0.8/len(ms)\n        for i, m in enumerate(ms):\n            ax.bar(x+i*w, bdf[m].astype(float), w, label=m.replace(\"nber_\",\"\").title(),\n                   color=[\"steelblue\",\"coral\",\"seagreen\",\"orchid\"][i%4], edgecolor=\"black\", lw=0.3)\n        ax.set_xticks(x+w*(len(ms)-1)/2); ax.set_xticklabels(bdf[\"method\"], rotation=15, ha=\"right\")\n        ax.set_ylabel(\"Score\"); ax.set_title(\"Baseline Comparison\"); ax.legend(); ax.set_ylim(0,1.05)\n        fig.savefig(f\"{FIGURES_DIR}/fig_baseline_comparison.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\nprint(f\"  Generated {n_figs} figures inline\")\n\n# ======================================================================\n# PART A2: MULTI-SEED ERROR BARS (5 seeds total, ~1.5 hours)\n# PRE requires error bars on main results. Run 4 additional seeds\n# for training + analysis only (baselines/rolling/figures use seed 42).\n# ======================================================================\nN_MAIN_SEEDS = 5\nEXTRA_SEEDS = [0, 1, 2, 3]  # + seed 42 from above = 5 total\n\nprint(f\"\\n{'#'*70}\")\nprint(f\"#  PART A2: MULTI-SEED ERROR BARS ({N_MAIN_SEEDS} seeds)\")\nprint(f\"#  Running {len(EXTRA_SEEDS)} additional seeds + seed 42 from above\")\nprint(f\"#  Only re-runs training + analysis (not baselines/rolling/figures)\")\nprint(f\"{'#'*70}\")\n\nmulti_seed_results = {}\n\n# Collect seed=42 results from the primary run\nfor mode_tag in [\"univariate\", \"multiasset\"]:\n    ap = f\"{RESULTS_DIR}/analysis_results_{mode_tag}.json\"\n    if os.path.exists(ap):\n        with open(ap) as f:\n            multi_seed_results.setdefault(mode_tag, {})[42] = json.load(f)\n\n# Run extra seeds\nfor seed in EXTRA_SEEDS:\n    seed_dir = f\"{OUTPUT_DIR}/seed_{seed}\"\n    seed_results = f\"{seed_dir}/results\"\n    seed_models = f\"{seed_dir}/models\"\n    os.makedirs(seed_results, exist_ok=True)\n    os.makedirs(seed_models, exist_ok=True)\n\n    for mode_tag in [\"univariate\", \"multiasset\"]:\n        # Check if already completed (resume-safe)\n        seed_ap = f\"{seed_results}/analysis_results_{mode_tag}.json\"\n        if os.path.exists(seed_ap):\n            print(f\"\\n  --- Seed {seed}, mode={mode_tag}: ALREADY DONE (resuming) ---\")\n            with open(seed_ap) as f:\n                multi_seed_results.setdefault(mode_tag, {})[seed] = json.load(f)\n            continue\n\n        print(f\"\\n  --- Seed {seed}, mode={mode_tag} ---\", flush=True)\n        ok = run(f'Seed {seed} {mode_tag}',\n            f'{python} {REPO_DIR}/experiments/run_main.py'\n            f' --config config/default.yaml --mode {mode_tag} --seed {seed}'\n            f' --output-dir {seed_dir}')\n\n        if os.path.exists(seed_ap):\n            with open(seed_ap) as f:\n                multi_seed_results.setdefault(mode_tag, {})[seed] = json.load(f)\n        else:\n            print(f\"  WARNING: No results for seed {seed} {mode_tag}\")\n\n# Aggregate and report\nMETRICS = [\n    'vamp2_score', 'spectral_gap', 'entropy_empirical', 'entropy_total',\n    'mean_irreversibility', 'detailed_balance_violation',\n    'fluctuation_theorem_ratio', 'n_complex_modes', 'complex_fraction',\n    'ktnd_nber_accuracy', 'ktnd_nber_f1',\n]\n\nmulti_seed_summary = {}\nfor mode_tag in [\"univariate\", \"multiasset\"]:\n    if mode_tag not in multi_seed_results:\n        continue\n    seed_data = multi_seed_results[mode_tag]\n    seeds_present = sorted(seed_data.keys())\n    print(f\"\\n  === {mode_tag.title()}: {len(seeds_present)} seeds ({seeds_present}) ===\")\n\n    summary = {'n_seeds': len(seeds_present), 'seeds': seeds_present}\n    for metric in METRICS:\n        vals = [seed_data[s].get(metric) for s in seeds_present\n                if seed_data[s].get(metric) is not None]\n        if vals:\n            vals = [float(v) for v in vals]\n            mean_val = np.mean(vals)\n            std_val = np.std(vals, ddof=1) if len(vals) > 1 else 0.0\n            summary[f'{metric}_mean'] = float(mean_val)\n            summary[f'{metric}_std'] = float(std_val)\n            print(f\"    {metric:35s}  {mean_val:.4f} +/- {std_val:.4f}  (n={len(vals)})\")\n\n    multi_seed_summary[mode_tag] = summary\n\n# Save aggregated results\nms_path = f\"{RESULTS_DIR}/multi_seed_summary.json\"\nwith open(ms_path, 'w') as f:\n    json.dump(multi_seed_summary, f, indent=2, default=str)\nprint(f\"\\n  Saved: {ms_path}\")\nresults['multi_seed'] = os.path.exists(ms_path)\n\n# ======================================================================\n# PART B: ABLATION STUDY (10 seeds, ~6-10 hours)\n# Uses STREAMING output so you can see progress live.\n# Saves incrementally -- if Colab disconnects, re-run and it resumes.\n# ======================================================================\nprint(f\"\\n{'#'*70}\")\nprint(f\"#  PART B: ABLATION STUDY ({N_ABLATION_SEEDS} seeds x ~32 variants)\")\nprint(f\"#  v1.4.0: shared_weights/no_entropy/no_spectral now correctly applied\")\nprint(f\"#  Output streams live. Saves after each variant (resume-safe).\")\nprint(f\"{'#'*70}\")\n\n# Use run_streaming for ablations -- output appears line-by-line\nresults['ablations'] = run_streaming(f'Ablations ({N_ABLATION_SEEDS} seeds)',\n    f'{python} -u experiments/run_ablations.py --config config/default.yaml'\n    f' --n-seeds {N_ABLATION_SEEDS} --n-jobs 1'\n    f' --output-dir {RESULTS_DIR}',\n    check_files=[f'{RESULTS_DIR}/ablation_summary.csv'])\n\nsummary_path = f\"{RESULTS_DIR}/ablation_summary.csv\"\nif os.path.exists(summary_path):\n    abl_df = pd.read_csv(summary_path)\n    print(f\"\\n  {len(abl_df)} ablation variants ({N_ABLATION_SEEDS} seeds each):\")\n    cols = ['name', 'n_valid', 'vamp2_mean', 'vamp2_std',\n            'spectral_gap_mean', 'spectral_gap_std',\n            'entropy_total_mean', 'entropy_total_std']\n    cols = [c for c in cols if c in abl_df.columns]\n    print(abl_df[cols].to_string(index=False))\n\n    if 'vamp2_mean' in abl_df.columns:\n        baseline = abl_df[abl_df['name'] == 'baseline']\n        if len(baseline) > 0:\n            bl_vamp2 = baseline['vamp2_mean'].values[0]\n            print(f\"\\n  Baseline VAMP-2: {bl_vamp2:.4f}\")\n            diff = abl_df.copy()\n            diff['vamp2_delta'] = ((diff['vamp2_mean'] - bl_vamp2) / abs(bl_vamp2) * 100)\n            notable = diff[abs(diff['vamp2_delta']) > 5].sort_values('vamp2_delta')\n            if len(notable) > 0:\n                print(f\"\\n  Variants with >5% VAMP-2 change from baseline:\")\n                for _, row in notable.iterrows():\n                    print(f\"    {row['name']:40s}  {row['vamp2_delta']:+.1f}%\")\n\n# ======================================================================\n# PART C: BROWNIAN GYRATOR BENCHMARK (~5 min)\n# ======================================================================\nprint(f\"\\n{'#'*70}\")\nprint(f\"#  PART C: BROWNIAN GYRATOR -- analytical EP benchmark\")\nprint(f\"#  2D coupled OU, T1!=T2 breaks detailed balance\")\nprint(f\"#  Exact EP: sigma = Tr[Q Sigma Q^T D^-1]\")\nprint(f\"{'#'*70}\")\n\nfrom scipy.linalg import solve_continuous_lyapunov\n\ndef analytical_ep(T1, T2, k=1.0, kappa=0.5):\n    A = np.array([[k, -kappa], [-kappa, k]])\n    D = np.array([[T1, 0.0], [0.0, T2]])\n    Sigma = solve_continuous_lyapunov(A, 2.0 * D)\n    Q = A - D @ np.linalg.inv(Sigma)\n    D_inv = np.diag([1.0/T1, 1.0/T2])\n    return np.trace(Q @ Sigma @ Q.T @ D_inv)\n\nprint(\"\\n  Analytical EP rates:\")\nfor T2 in [1.0, 1.5, 3.0, 5.0]:\n    ep = analytical_ep(1.0, T2)\n    print(f\"    T1=1.0, T2={T2:.1f}:  EP = {ep:.6f}  {'(equilibrium)' if T2 == 1.0 else ''}\")\n\nresults['gyrator'] = run('Brownian gyrator tests',\n    f'{python} -m pytest tests/test_synthetic.py::TestBrownianGyrator -v')\n\n# ======================================================================\n# FINAL REPORT\n# ======================================================================\n\nprint(f\"\\n{'='*70}\")\nprint(f\"  ALL OUTPUT FILES\")\nprint(f\"{'='*70}\")\nfor dirpath, dirnames, filenames in os.walk(OUTPUT_DIR):\n    for f in sorted(filenames):\n        fp = os.path.join(dirpath, f)\n        sz = os.path.getsize(fp)\n        rel = os.path.relpath(fp, OUTPUT_DIR)\n        print(f\"  {sz:>10,} bytes  {rel}\")\n\ntotal_min = (time.time() - pipeline_start) / 60\nn_ok = sum(v for v in results.values() if isinstance(v, bool) and v)\nn_total = len(results)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"  COMPLETE: {n_ok}/{n_total} stages passed ({total_min:.1f} min total)\")\nprint(f\"  Version: v1.5.0\")\nprint(f\"{'='*70}\")\nfor name, ok in results.items():\n    print(f\"  {'OK' if ok else 'FAIL':6s}  {name}\")\n\n# Print single-seed results for BOTH modes\nfor mode_tag, label in [(\"univariate\", \"Univariate (SPY)\"), (\"multiasset\", \"Multiasset (11 ETFs)\")]:\n    ap = f\"{RESULTS_DIR}/analysis_results_{mode_tag}.json\"\n    if not os.path.exists(ap):\n        continue\n    with open(ap) as f:\n        r = json.load(f)\n    print(f\"\\n  === {label} (seed 42) ===\")\n    print(f\"    VAMP-2 score:         {r.get('vamp2_score', 'N/A')}\")\n    print(f\"    Spectral gap:         {r.get('spectral_gap', 'N/A')}\")\n    print(f\"    Entropy (empirical):  {r.get('entropy_empirical', 'N/A')} \"\n          f\"[{r.get('entropy_ci_lower', '?')}, {r.get('entropy_ci_upper', '?')}] 95% CI\")\n    print(f\"    Spectral entropy:     {r.get('entropy_total', 'N/A')}\")\n    print(f\"    Mean irreversibility: {r.get('mean_irreversibility', 'N/A')}\")\n    print(f\"    Irrev method:         {r.get('irrev_method', 'N/A')}\")\n    print(f\"    DB violation:         {r.get('detailed_balance_violation', 'N/A')}\")\n    print(f\"    Complex modes:        {r.get('n_complex_modes', 'N/A')}/{r.get('n_modes', 'N/A')}\")\n    print(f\"    FT ratio:             {r.get('fluctuation_theorem_ratio', 'N/A')}\")\n\n    ktnd_acc = r.get('ktnd_nber_accuracy')\n    if ktnd_acc is not None:\n        print(f\"    KTND NBER accuracy:   {ktnd_acc:.3f}\")\n        print(f\"    KTND NBER F1:         {r.get('ktnd_nber_f1', 'N/A'):.3f}\")\n        print(f\"    KTND naive accuracy:  {r.get('ktnd_naive_accuracy', 'N/A'):.3f}\")\n        print(f\"    Mean regime duration: {r.get('ktnd_mean_regime_duration', 'N/A'):.1f} days\")\n\n# Print multi-seed aggregated results\nms_path = f\"{RESULTS_DIR}/multi_seed_summary.json\"\nif os.path.exists(ms_path):\n    with open(ms_path) as f:\n        ms = json.load(f)\n    print(f\"\\n  === MULTI-SEED SUMMARY (mean +/- std) ===\")\n    for mode_tag in [\"univariate\", \"multiasset\"]:\n        if mode_tag not in ms:\n            continue\n        s = ms[mode_tag]\n        label = \"Univariate (SPY)\" if mode_tag == \"univariate\" else \"Multiasset (11 ETFs)\"\n        print(f\"\\n  {label} ({s.get('n_seeds', '?')} seeds):\")\n        for metric in METRICS:\n            mk, sk = f'{metric}_mean', f'{metric}_std'\n            if mk in s:\n                print(f\"    {metric:35s}  {s[mk]:.4f} +/- {s[sk]:.4f}\")\n\nstat_path = f\"{RESULTS_DIR}/statistical_tests.json\"\nif os.path.exists(stat_path):\n    with open(stat_path) as f:\n        st = json.load(f)\n    print(f\"\\n  === Statistical Tests ===\")\n    for k, v in st.items():\n        if isinstance(v, dict):\n            if v.get('skipped'):\n                print(f\"    {k}: SKIPPED ({v.get('reason', '')})\")\n            else:\n                pval = v.get('p_value', v.get('pvalue', None))\n                if pval is not None:\n                    print(f\"    {k}: p={pval:.4f}\")\n        elif isinstance(v, (int, float)):\n            print(f\"    {k}: {v}\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"  DONE. Total wall time: {total_min:.1f} min\")\nprint(f\"  Next: Cell 3 (view figures) -> Cell 4 (download zip)\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "code",
   "source": "#@title 2b. MINIMAL RE-RUN: KTND HMM fix + ablations + gyrator (run AFTER Cell 2 completed Part A)\n#\n# This cell re-runs ONLY what's needed after the v1.5.1 HMM regime fix:\n#   1. Re-detect regimes from saved eigenfunctions (no retraining, ~1 min)\n#   2. Run ablations with resume (skips completed variants, ~6 hours)\n#   3. Brownian gyrator benchmark (~5 min)\n#\n# Prerequisites: Cell 2 must have completed Part A + Part A2 (training + multi-seed).\n# If ablations were interrupted, this cell resumes from where they left off.\n\nimport subprocess, time, json, os, sys\nimport numpy as np\nimport pandas as pd\n\nREPO_DIR = \"/content/ktnd_finance\"\nOUTPUT_DIR = \"/content/ktnd_finance/outputs\"\nRESULTS_DIR = \"/content/ktnd_finance/outputs/results\"\nN_ABLATION_SEEDS = 10\n\nos.chdir(REPO_DIR)\nsys.path.insert(0, REPO_DIR)\npython = sys.executable\n\n# Pull latest code (includes HMM regime detection fix)\nprint(\"Pulling latest code...\")\n!cd {REPO_DIR} && git pull\n\ndef run(name, cmd, check_files=None):\n    print(f\"\\n{'='*70}\")\n    print(f\"  STAGE: {name}\")\n    print(f\"  CMD: {cmd}\")\n    print(f\"{'='*70}\")\n    t0 = time.time()\n    result = subprocess.run(cmd, shell=True, cwd=REPO_DIR, capture_output=True, text=True)\n    elapsed = time.time() - t0\n    if result.stdout:\n        for line in result.stdout.strip().split('\\n'):\n            print(f\"  {line}\")\n    if result.returncode != 0:\n        if result.stderr:\n            for line in result.stderr.strip().split('\\n')[-20:]:\n                print(f\"  ! {line}\")\n        print(f\"  >> {name}: FAILED ({elapsed/60:.1f} min)\")\n        return False\n    if check_files:\n        missing = [f for f in check_files if not os.path.exists(f)]\n        if missing:\n            for f in missing:\n                print(f\"    MISSING: {f}\")\n            return False\n    print(f\"  >> {name}: OK ({elapsed/60:.1f} min)\")\n    return True\n\ndef run_streaming(name, cmd, check_files=None):\n    print(f\"\\n{'='*70}\")\n    print(f\"  STAGE: {name}\")\n    print(f\"  CMD: {cmd}\")\n    print(f\"{'='*70}\", flush=True)\n    t0 = time.time()\n    proc = subprocess.Popen(cmd, shell=True, cwd=REPO_DIR,\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n    for line in proc.stdout:\n        print(f\"  {line}\", end=\"\", flush=True)\n    proc.wait()\n    elapsed = time.time() - t0\n    if proc.returncode != 0:\n        print(f\"  >> {name}: FAILED ({elapsed/60:.1f} min)\")\n        return False\n    if check_files:\n        missing = [f for f in check_files if not os.path.exists(f)]\n        if missing:\n            return False\n    print(f\"  >> {name}: OK ({elapsed/60:.1f} min)\")\n    return True\n\nt_start = time.time()\nresults = {}\n\n# ======================================================================\n# STEP 1: RE-DETECT REGIMES WITH HMM (from saved eigenfunctions)\n# No retraining needed -- loads .npy files and re-runs detection\n# ======================================================================\nprint(f\"\\n{'#'*70}\")\nprint(f\"#  STEP 1: KTND REGIME RE-DETECTION (HMM on eigenfunctions)\")\nprint(f\"#  Uses saved eigenfunctions -- no GPU/retraining needed\")\nprint(f\"{'#'*70}\")\n\nfrom src.analysis.regime import RegimeDetector\nfrom src.constants import NBER_RECESSIONS, DATE_RANGES\n\n# Re-detect for primary run (seed 42) + all multi-seed runs\nall_result_dirs = [(\"seed_42\", RESULTS_DIR)]\nfor seed in [0, 1, 2, 3]:\n    sd = f\"{OUTPUT_DIR}/seed_{seed}/results\"\n    if os.path.exists(sd):\n        all_result_dirs.append((f\"seed_{seed}\", sd))\n\nfor seed_label, rdir in all_result_dirs:\n    for mode_tag in [\"univariate\", \"multiasset\"]:\n        efunc_path = f\"{rdir}/eigenfunctions_right.npy\"\n        results_path = f\"{rdir}/analysis_results_{mode_tag}.json\"\n\n        # For seed_42, eigenfunctions are in main results dir\n        # For multi-seed, they're in seed_X/results/\n        # But eigenfunctions are mode-agnostic -- they get overwritten by the last mode run\n        # Only re-detect for the mode whose eigenfunctions are current\n        if not os.path.exists(efunc_path) or not os.path.exists(results_path):\n            continue\n\n        print(f\"\\n  --- {seed_label} / {mode_tag} ---\")\n        u_np = np.load(efunc_path)\n        print(f\"    Eigenfunctions shape: {u_np.shape}\")\n\n        # Load dates from regime labels CSV or prices\n        dates_csv = f\"{rdir}/ktnd_regime_labels.csv\"\n        if os.path.exists(dates_csv):\n            dates_df = pd.read_csv(dates_csv)\n            regime_dates = pd.to_datetime(dates_df[\"date\"])\n        else:\n            # Fall back to prices.csv dates\n            prices_df = pd.read_csv(f\"{REPO_DIR}/data/prices.csv\")\n            regime_dates = pd.to_datetime(prices_df[\"Date\"])[:len(u_np)]\n\n        if len(regime_dates) < len(u_np):\n            regime_dates = regime_dates[:len(u_np)]\n        elif len(regime_dates) > len(u_np):\n            regime_dates = regime_dates[:len(u_np)]\n\n        # Run HMM detection\n        regime_labels = RegimeDetector.detect_from_eigenfunctions(\n            u_np, n_regimes=2, method=\"hmm\",\n        )\n\n        # Compare with NBER\n        nber_comparison = RegimeDetector.compare_with_nber(\n            regime_labels, regime_dates,\n            nber_recessions=NBER_RECESSIONS,\n            train_end=DATE_RANGES[\"train\"][1],\n        )\n\n        durations = RegimeDetector.compute_regime_durations(regime_labels)\n\n        print(f\"    HMM accuracy:  {nber_comparison['accuracy']:.3f} (naive: {nber_comparison['naive_accuracy']:.3f})\")\n        print(f\"    HMM F1:        {nber_comparison['f1']:.3f}\")\n        print(f\"    HMM precision: {nber_comparison['precision']:.3f}\")\n        print(f\"    HMM recall:    {nber_comparison['recall']:.3f}\")\n        print(f\"    Transitions:   {np.sum(np.diff(regime_labels) != 0)}\")\n        print(f\"    Mean duration: {np.mean(durations):.1f} days\")\n\n        # Update the results JSON\n        with open(results_path) as f:\n            r = json.load(f)\n\n        r[\"ktnd_nber_accuracy\"] = nber_comparison[\"accuracy\"]\n        r[\"ktnd_nber_precision\"] = nber_comparison[\"precision\"]\n        r[\"ktnd_nber_recall\"] = nber_comparison[\"recall\"]\n        r[\"ktnd_nber_f1\"] = nber_comparison[\"f1\"]\n        r[\"ktnd_naive_accuracy\"] = nber_comparison[\"naive_accuracy\"]\n        r[\"ktnd_recession_label\"] = int(nber_comparison[\"recession_label\"])\n        r[\"ktnd_detection_method\"] = \"hmm\"\n        r[\"ktnd_mean_regime_duration\"] = float(np.mean(durations))\n        r[\"ktnd_n_regimes_detected\"] = len(set(regime_labels))\n\n        with open(results_path, 'w') as f:\n            json.dump(r, f, indent=2, default=str)\n\n        # Also update generic analysis_results.json if this is seed_42\n        if seed_label == \"seed_42\":\n            generic_path = f\"{rdir}/analysis_results.json\"\n            if os.path.exists(generic_path):\n                with open(generic_path) as f:\n                    rg = json.load(f)\n                rg.update({k: v for k, v in r.items() if k.startswith(\"ktnd_\")})\n                with open(generic_path, 'w') as f:\n                    json.dump(rg, f, indent=2, default=str)\n\n        # Save updated regime labels\n        regime_df = pd.DataFrame({\n            \"date\": regime_dates[:len(regime_labels)],\n            \"regime_label\": regime_labels[:len(regime_dates)],\n        })\n        regime_df.to_csv(f\"{rdir}/ktnd_regime_labels.csv\", index=False)\n\nresults['ktnd_redetect'] = True\nprint(f\"\\n  Regime re-detection complete.\")\n\n# Re-aggregate multi-seed summary with updated KTND metrics\nprint(f\"\\n  Re-aggregating multi-seed summary...\")\nMETRICS = [\n    'vamp2_score', 'spectral_gap', 'entropy_empirical', 'entropy_total',\n    'mean_irreversibility', 'detailed_balance_violation',\n    'fluctuation_theorem_ratio', 'n_complex_modes', 'complex_fraction',\n    'ktnd_nber_accuracy', 'ktnd_nber_f1',\n]\n\nmulti_seed_results = {}\nfor mode_tag in [\"univariate\", \"multiasset\"]:\n    for seed, rdir in [(42, RESULTS_DIR)] + [(s, f\"{OUTPUT_DIR}/seed_{s}/results\") for s in [0,1,2,3]]:\n        ap = f\"{rdir}/analysis_results_{mode_tag}.json\"\n        if os.path.exists(ap):\n            with open(ap) as f:\n                multi_seed_results.setdefault(mode_tag, {})[seed] = json.load(f)\n\nmulti_seed_summary = {}\nfor mode_tag in [\"univariate\", \"multiasset\"]:\n    if mode_tag not in multi_seed_results:\n        continue\n    seed_data = multi_seed_results[mode_tag]\n    seeds_present = sorted(seed_data.keys())\n    print(f\"\\n  === {mode_tag.title()}: {len(seeds_present)} seeds ===\")\n    summary = {'n_seeds': len(seeds_present), 'seeds': seeds_present}\n    for metric in METRICS:\n        vals = [seed_data[s].get(metric) for s in seeds_present\n                if seed_data[s].get(metric) is not None]\n        if vals:\n            vals = [float(v) for v in vals]\n            mean_val = np.mean(vals)\n            std_val = np.std(vals, ddof=1) if len(vals) > 1 else 0.0\n            summary[f'{metric}_mean'] = float(mean_val)\n            summary[f'{metric}_std'] = float(std_val)\n            print(f\"    {metric:35s}  {mean_val:.4f} +/- {std_val:.4f}\")\n    multi_seed_summary[mode_tag] = summary\n\nms_path = f\"{RESULTS_DIR}/multi_seed_summary.json\"\nwith open(ms_path, 'w') as f:\n    json.dump(multi_seed_summary, f, indent=2, default=str)\nprint(f\"  Saved: {ms_path}\")\n\n# ======================================================================\n# STEP 2: ABLATIONS (resume-safe, ~6 hours)\n# ======================================================================\nprint(f\"\\n{'#'*70}\")\nprint(f\"#  STEP 2: ABLATION STUDY ({N_ABLATION_SEEDS} seeds)\")\nprint(f\"#  Resume-safe: skips already-completed variants\")\nprint(f\"{'#'*70}\")\n\nresults['ablations'] = run_streaming(f'Ablations ({N_ABLATION_SEEDS} seeds)',\n    f'{python} -u experiments/run_ablations.py --config config/default.yaml'\n    f' --n-seeds {N_ABLATION_SEEDS} --n-jobs 1'\n    f' --output-dir {RESULTS_DIR}',\n    check_files=[f'{RESULTS_DIR}/ablation_summary.csv'])\n\nsummary_path = f\"{RESULTS_DIR}/ablation_summary.csv\"\nif os.path.exists(summary_path):\n    abl_df = pd.read_csv(summary_path)\n    print(f\"\\n  {len(abl_df)} ablation variants completed:\")\n    cols = ['name', 'n_valid', 'vamp2_mean', 'vamp2_std']\n    cols = [c for c in cols if c in abl_df.columns]\n    print(abl_df[cols].to_string(index=False))\n\n# ======================================================================\n# STEP 3: BROWNIAN GYRATOR BENCHMARK (~5 min)\n# ======================================================================\nprint(f\"\\n{'#'*70}\")\nprint(f\"#  STEP 3: BROWNIAN GYRATOR BENCHMARK\")\nprint(f\"{'#'*70}\")\n\nfrom scipy.linalg import solve_continuous_lyapunov\n\ndef analytical_ep(T1, T2, k=1.0, kappa=0.5):\n    A = np.array([[k, -kappa], [-kappa, k]])\n    D = np.array([[T1, 0.0], [0.0, T2]])\n    Sigma = solve_continuous_lyapunov(A, 2.0 * D)\n    Q = A - D @ np.linalg.inv(Sigma)\n    D_inv = np.diag([1.0/T1, 1.0/T2])\n    return np.trace(Q @ Sigma @ Q.T @ D_inv)\n\nprint(\"\\n  Analytical EP rates:\")\nfor T2 in [1.0, 1.5, 3.0, 5.0]:\n    ep = analytical_ep(1.0, T2)\n    print(f\"    T1=1.0, T2={T2:.1f}:  EP = {ep:.6f}  {'(equilibrium)' if T2 == 1.0 else ''}\")\n\nresults['gyrator'] = run('Brownian gyrator tests',\n    f'{python} -m pytest tests/test_synthetic.py::TestBrownianGyrator -v')\n\n# ======================================================================\n# SUMMARY\n# ======================================================================\ntotal_min = (time.time() - t_start) / 60\nprint(f\"\\n{'='*70}\")\nprint(f\"  MINIMAL RE-RUN COMPLETE ({total_min:.1f} min)\")\nprint(f\"{'='*70}\")\nfor name, ok in results.items():\n    print(f\"  {'OK' if ok else 'FAIL':6s}  {name}\")\n\n# Show updated KTND results\nfor mode_tag, label in [(\"univariate\", \"Univariate\"), (\"multiasset\", \"Multiasset\")]:\n    ap = f\"{RESULTS_DIR}/analysis_results_{mode_tag}.json\"\n    if os.path.exists(ap):\n        with open(ap) as f:\n            r = json.load(f)\n        acc = r.get('ktnd_nber_accuracy')\n        if acc is not None:\n            print(f\"\\n  {label} KTND (HMM): acc={acc:.3f}  F1={r.get('ktnd_nber_f1', 0):.3f}  \"\n                  f\"naive={r.get('ktnd_naive_accuracy', 0):.3f}  \"\n                  f\"duration={r.get('ktnd_mean_regime_duration', 0):.0f}d  \"\n                  f\"method={r.get('ktnd_detection_method', '?')}\")\n\nprint(f\"\\n  Next: Cell 3 (view figures) -> Cell 4 (download)\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3. View figures (run after Cell 2 finishes)\n\nimport glob, os\nfrom IPython.display import Image, display\n\nFIGURES_DIR = \"/content/ktnd_finance/outputs/figures\"\n\npngs = sorted(glob.glob(f\"{FIGURES_DIR}/*.png\"))\nsup_dir = os.path.join(FIGURES_DIR, \"supplemental\")\nif os.path.exists(sup_dir):\n    pngs += sorted(glob.glob(f\"{sup_dir}/*.png\"))\n\nif pngs:\n    print(f\"Found {len(pngs)} figures:\\n\")\n    for p in pngs:\n        print(f\"--- {os.path.basename(p)} ---\")\n        display(Image(filename=p, width=800))\n        print()\nelse:\n    print(\"No figures found. Make sure Cell 2 has finished running first.\")\n    print(f\"Checked: {FIGURES_DIR}\")\n    results_dir = \"/content/ktnd_finance/outputs/results\"\n    if os.path.exists(results_dir):\n        files = os.listdir(results_dir)\n        print(f\"Result files available ({len(files)}): {files}\")\n    else:\n        print(\"No results directory found - Cell 2 needs to run first.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Download all results as zip\n",
    "\n",
    "!cd /content/ktnd_finance && zip -rq /content/ktnd_results.zip outputs/\n",
    "from google.colab import files\n",
    "files.download('/content/ktnd_results.zip')\n",
    "print(\"Download started.\")"
   ]
  }
 ]
}