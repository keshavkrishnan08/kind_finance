{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# KTND-Finance v1.6.0\n\n1. **Cell 1** -- Setup (~2 min)\n2. **Cell 2** -- Full pipeline + multi-seed (~2-3 hours)\n3. **Cell 3** -- Ablations (~6 hours, resume-safe)\n4. **Cell 4** -- View figures\n5. **Cell 5** -- Download zip\n\n**GPU required**: Runtime -> Change runtime type -> T4 GPU\n\n**Resume**: If Colab disconnects, change `RESUME_FROM` in Cell 2 to skip completed stages:\n- `1` = start fresh (default)\n- `4` = skip to multiasset (tests/download/univariate done)\n- `5` = skip to baselines (both models trained)\n- `7` = skip to robustness (baselines+rolling done)\n- `10` = skip to entropy calibration\n- `11` = skip to figures only\n- `12` = skip to multi-seed only"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup (install + clone + verify) - ~2 min\n",
    "\n",
    "# Install missing dependencies (torch/numpy/pandas/scipy/sklearn/matplotlib are pre-installed)\n",
    "!pip install -q yfinance>=1.0.0 hmmlearn>=0.3.0 statsmodels>=0.14.0 arch>=6.0.0 pyyaml>=6.0\n",
    "\n",
    "# Clone repo\n",
    "import os, sys\n",
    "REPO_URL = \"https://github.com/keshavkrishnan08/kind_finance.git\"\n",
    "REPO_DIR = \"/content/ktnd_finance\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !cd {REPO_DIR} && git pull\n",
    "else:\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Verify\n",
    "import torch, numpy as np\n",
    "from src.model.vampnet import NonEquilibriumVAMPNet\n",
    "print(f\"Python {sys.version.split()[0]} | PyTorch {torch.__version__} | \"\n",
    "      f\"CUDA: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2. Run pipeline + multi-seed + figures (~2-3 hours)\n\nimport subprocess, time, json, os, sys, glob\n\n# ==========================================================================\n# RESUME CONTROL -- change this to skip completed stages\n# ==========================================================================\nRESUME_FROM = 1  #@param {type:\"integer\"}\n# 1=fresh, 4=multiasset, 5=baselines, 7=robustness, 11=figures, 12=multi-seed\n\n# ==========================================================================\n# PATHS\n# ==========================================================================\nREPO_DIR = \"/content/ktnd_finance\"\nOUTPUT_DIR = \"/content/ktnd_finance/outputs\"\nRESULTS_DIR = \"/content/ktnd_finance/outputs/results\"\nMODELS_DIR = \"/content/ktnd_finance/outputs/models\"\nFIGURES_DIR = \"/content/ktnd_finance/outputs/figures\"\nDATA_DIR = \"/content/ktnd_finance/data\"\n\nfor d in [OUTPUT_DIR, RESULTS_DIR, MODELS_DIR, FIGURES_DIR]:\n    os.makedirs(d, exist_ok=True)\n\nos.chdir(REPO_DIR)\npython = sys.executable\n\ndef run(name, cmd, check_files=None):\n    \"\"\"Run a stage, print output, verify files.\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"  STAGE: {name}\")\n    print(f\"{'='*70}\")\n    t0 = time.time()\n    result = subprocess.run(cmd, shell=True, cwd=REPO_DIR,\n                            capture_output=True, text=True)\n    elapsed = time.time() - t0\n\n    if result.stdout:\n        for line in result.stdout.strip().split('\\n')[-40:]:\n            print(f\"  {line}\")\n\n    if result.returncode != 0:\n        print(f\"\\n  === STDERR (last 30 lines) ===\")\n        if result.stderr:\n            for line in result.stderr.strip().split('\\n')[-30:]:\n                print(f\"  ! {line}\")\n        print(f\"  >> {name}: FAILED (exit {result.returncode}, {elapsed/60:.1f} min)\")\n        return False\n\n    if check_files:\n        missing = [f for f in check_files if not os.path.exists(f)]\n        if missing:\n            for f in missing:\n                print(f\"    MISSING: {f}\")\n            if result.stderr:\n                for line in result.stderr.strip().split('\\n')[-15:]:\n                    print(f\"  ! {line}\")\n            print(f\"  >> {name}: INCOMPLETE ({elapsed/60:.1f} min)\")\n            return False\n        for f in check_files:\n            sz = os.path.getsize(f)\n            print(f\"  OK: {os.path.basename(f)} ({sz:,} bytes)\")\n\n    print(f\"  >> {name}: OK ({elapsed/60:.1f} min)\")\n    return True\n\ndef skip(stage_num, name, check_files=None):\n    \"\"\"Check if stage should be skipped. Returns True if skipped.\"\"\"\n    if stage_num >= RESUME_FROM:\n        return False\n    if check_files:\n        missing = [f for f in check_files if not os.path.exists(f)]\n        if missing:\n            print(f\"\\n  STAGE {stage_num} ({name}): Cannot skip -- missing files: {[os.path.basename(f) for f in missing]}\")\n            return False\n    print(f\"  STAGE {stage_num} ({name}): SKIPPED (RESUME_FROM={RESUME_FROM})\")\n    return True\n\npipeline_start = time.time()\nresults = {}\n\nif RESUME_FROM > 1:\n    print(f\"RESUMING FROM STAGE {RESUME_FROM} (skipping stages 1-{RESUME_FROM-1})\")\n\n# ======================================================================\n# STAGE 1: Quick tests\n# ======================================================================\nif skip(1, 'Quick tests'):\n    results['tests'] = True\nelse:\n    results['tests'] = run('Quick tests',\n        f'{python} -m pytest tests/ -q --tb=short -k \"not test_synthetic\"')\n\n# ======================================================================\n# STAGE 2: Download data\n# ======================================================================\nif skip(2, 'Download data', [f'{DATA_DIR}/prices.csv', f'{DATA_DIR}/vix.csv']):\n    results['download'] = True\nelse:\n    results['download'] = run('Download data',\n        f'{python} {REPO_DIR}/data/download.py --mode all',\n        check_files=[f'{DATA_DIR}/prices.csv', f'{DATA_DIR}/vix.csv'])\n\n# ======================================================================\n# STAGE 3: Train univariate\n# ======================================================================\nuni_files = [\n    f'{RESULTS_DIR}/analysis_results.json',\n    f'{RESULTS_DIR}/analysis_results_univariate.json',\n    f'{RESULTS_DIR}/eigenvalues.csv',\n    f'{RESULTS_DIR}/entropy_decomposition.csv',\n    f'{RESULTS_DIR}/irreversibility_field.npy',\n    f'{MODELS_DIR}/vampnet_univariate.pt',\n    f'{RESULTS_DIR}/training_history_univariate.json',\n]\nif skip(3, 'Train univariate', uni_files):\n    results['train_uni'] = True\nelse:\n    results['train_uni'] = run('Train univariate (SPY)',\n        f'{python} {REPO_DIR}/experiments/run_main.py'\n        f' --config config/default.yaml --mode univariate --seed 42'\n        f' --output-dir {OUTPUT_DIR}',\n        check_files=uni_files)\n\n# ======================================================================\n# STAGE 4: Train multiasset\n# ======================================================================\nmulti_files = [\n    f'{RESULTS_DIR}/analysis_results_multiasset.json',\n    f'{MODELS_DIR}/vampnet_multiasset.pt',\n    f'{RESULTS_DIR}/training_history_multiasset.json',\n]\nif skip(4, 'Train multiasset', multi_files):\n    results['train_multi'] = True\nelse:\n    results['train_multi'] = run('Train multiasset (11 ETFs)',\n        f'{python} {REPO_DIR}/experiments/run_main.py'\n        f' --config config/default.yaml --mode multiasset --seed 42'\n        f' --output-dir {OUTPUT_DIR}',\n        check_files=multi_files)\n\n# ======================================================================\n# STAGE 5: Baselines\n# ======================================================================\nif skip(5, 'Baselines', [f'{RESULTS_DIR}/baseline_comparison.csv']):\n    results['baselines'] = True\nelse:\n    results['baselines'] = run('Baselines',\n        f'{python} {REPO_DIR}/experiments/run_baselines.py'\n        f' --config config/default.yaml --output-dir {RESULTS_DIR}',\n        check_files=[f'{RESULTS_DIR}/baseline_comparison.csv'])\n\n# ======================================================================\n# STAGE 6: Rolling\n# ======================================================================\nif skip(6, 'Rolling', [f'{RESULTS_DIR}/spectral_gap_timeseries.csv']):\n    results['rolling'] = True\nelse:\n    results['rolling'] = run('Rolling spectral analysis',\n        f'{python} {REPO_DIR}/experiments/run_rolling.py'\n        f' --config config/default.yaml --mode univariate'\n        f' --checkpoint {MODELS_DIR}/vampnet_univariate.pt'\n        f' --output-dir {RESULTS_DIR}',\n        check_files=[f'{RESULTS_DIR}/spectral_gap_timeseries.csv'])\n\n# ======================================================================\n# STAGE 7: Robustness univariate (IAAFT)\n# ======================================================================\nif skip(7, 'Robustness univariate', [f'{RESULTS_DIR}/statistical_tests.json']):\n    results['robustness_uni'] = True\nelse:\n    results['robustness_uni'] = run('Robustness (univariate, IAAFT)',\n        f'{python} {REPO_DIR}/experiments/run_robustness.py'\n        f' --config config/default.yaml --mode univariate'\n        f' --checkpoint {MODELS_DIR}/vampnet_univariate.pt'\n        f' --output-dir {RESULTS_DIR}',\n        check_files=[f'{RESULTS_DIR}/statistical_tests.json'])\n\n# ======================================================================\n# STAGE 8: Robustness multiasset (IAAFT)\n# ======================================================================\nif skip(8, 'Robustness multiasset', [f'{RESULTS_DIR}/statistical_tests_multiasset.json']):\n    results['robustness_multi'] = True\nelse:\n    results['robustness_multi'] = run('Robustness (multiasset, IAAFT)',\n        f'{python} {REPO_DIR}/experiments/run_robustness.py'\n        f' --config config/default.yaml --mode multiasset'\n        f' --checkpoint {MODELS_DIR}/vampnet_multiasset.pt'\n        f' --output-dir {RESULTS_DIR}',\n        check_files=[f'{RESULTS_DIR}/statistical_tests_multiasset.json'])\n\n# ======================================================================\n# STAGE 9: Walk-forward cross-validation\n# ======================================================================\nfor mode_tag in [\"univariate\", \"multiasset\"]:\n    cv_file = f'{RESULTS_DIR}/cv_results_{mode_tag}.json'\n    if skip(9, f'CV {mode_tag}', [cv_file]):\n        results[f'cv_{mode_tag}'] = True\n    else:\n        results[f'cv_{mode_tag}'] = run(f'Walk-forward CV ({mode_tag})',\n            f'{python} {REPO_DIR}/experiments/run_cv.py'\n            f' --config config/default.yaml --mode {mode_tag} --n-folds 5'\n            f' --output-dir {RESULTS_DIR}',\n            check_files=[cv_file])\n\n# ======================================================================\n# STAGE 10: Entropy calibration\n# ======================================================================\nif skip(10, 'Entropy calibration', [f'{RESULTS_DIR}/entropy_calibration.json']):\n    results['entropy_cal'] = True\nelse:\n    results['entropy_cal'] = run('Entropy calibration (Brownian gyrator)',\n        f'{python} {REPO_DIR}/experiments/run_entropy_calibration.py'\n        f' --output-dir {RESULTS_DIR} --n-steps 50000',\n        check_files=[f'{RESULTS_DIR}/entropy_calibration.json'])\n\n# ======================================================================\n# STAGE 11: Figures\n# ======================================================================\nif skip(11, 'Figures'):\n    results['figures'] = True\nelse:\n    results['figures'] = run('Generate figures',\n        f'{python} {REPO_DIR}/experiments/run_figures.py'\n        f' --results-dir {RESULTS_DIR} --figures-dir {FIGURES_DIR}')\n\n    # Inline fallback figures\n    import matplotlib\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n\n    n_figs = 0\n\n    for mode_tag, label in [(\"univariate\", \"Univariate (SPY)\"), (\"multiasset\", \"Multiasset\")]:\n        ap = f\"{RESULTS_DIR}/analysis_results_{mode_tag}.json\"\n        if not os.path.exists(ap):\n            continue\n        with open(ap) as f:\n            ar = json.load(f)\n        er, ei = ar.get(\"eigenvalues_real\"), ar.get(\"eigenvalues_imag\")\n        if er and ei:\n            er, ei = np.array(er), np.array(ei)\n            mags = np.sqrt(er**2 + ei**2)\n            fig, ax = plt.subplots(figsize=(7,7))\n            th = np.linspace(0, 2*np.pi, 300)\n            ax.plot(np.cos(th), np.sin(th), \"k--\", lw=0.8, alpha=0.5)\n            sc = ax.scatter(er, ei, c=mags, cmap=\"viridis\", edgecolors=\"k\", linewidths=0.4, s=80, zorder=3)\n            plt.colorbar(sc, ax=ax, label=\"|$\\\\lambda$|\")\n            for i, idx in enumerate(np.argsort(-mags)[:5]):\n                ax.annotate(f\"$\\\\lambda_{i}$\", (er[idx], ei[idx]), textcoords=\"offset points\", xytext=(8,8), fontsize=9)\n            ax.set_xlabel(\"Re($\\\\lambda$)\"); ax.set_ylabel(\"Im($\\\\lambda$)\")\n            ax.set_title(f\"Koopman Eigenvalue Spectrum -- {label}\"); ax.set_aspect(\"equal\"); ax.grid(True, alpha=0.3)\n            fig.savefig(f\"{FIGURES_DIR}/fig1_eigenvalue_spectrum_{mode_tag}.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\n    for csv_name, title, ycol in [\n        (\"eigenvalues.csv\", \"Eigenvalue Magnitudes\", \"magnitude\"),\n        (\"entropy_decomposition.csv\", \"Entropy Decomposition\", \"entropy_production\"),\n    ]:\n        p = f\"{RESULTS_DIR}/{csv_name}\"\n        if os.path.exists(p):\n            df = pd.read_csv(p)\n            if ycol in df.columns:\n                fig, ax = plt.subplots(figsize=(8,5))\n                ax.bar(df[\"mode\"], df[ycol], color=\"coral\" if \"entropy\" in csv_name else \"steelblue\", edgecolor=\"black\", lw=0.3)\n                ax.set_xlabel(\"Mode\"); ax.set_ylabel(ycol); ax.set_title(title); ax.grid(True, alpha=0.3, axis=\"y\")\n                fig.savefig(f\"{FIGURES_DIR}/fig_{csv_name.replace('.csv','')}.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\n    irp = f\"{RESULTS_DIR}/irreversibility_field.npy\"\n    if os.path.exists(irp):\n        ir = np.load(irp, allow_pickle=True)\n        fig, ax = plt.subplots(figsize=(14,4))\n        ax.fill_between(range(len(ir)), ir, alpha=0.4, color=\"darkorange\"); ax.plot(ir, lw=0.5, color=\"darkorange\")\n        ax.set_xlabel(\"Time\"); ax.set_ylabel(\"$I(x)$\"); ax.set_title(\"Irreversibility Field\"); ax.grid(True, alpha=0.3)\n        fig.savefig(f\"{FIGURES_DIR}/fig_irreversibility_field.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\n    rcp = f\"{RESULTS_DIR}/spectral_gap_timeseries.csv\"\n    if os.path.exists(rcp):\n        rdf = pd.read_csv(rcp)\n        if \"spectral_gap\" in rdf.columns:\n            fig, ax = plt.subplots(figsize=(14,5))\n            x = pd.to_datetime(rdf[\"center_date\"]) if \"center_date\" in rdf.columns else range(len(rdf))\n            ax.plot(x, rdf[\"spectral_gap\"], color=\"steelblue\", lw=1.0)\n            ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Spectral Gap\"); ax.set_title(\"Rolling Spectral Gap\"); ax.grid(True, alpha=0.3)\n            fig.savefig(f\"{FIGURES_DIR}/fig_spectral_gap.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\n    bcp = f\"{RESULTS_DIR}/baseline_comparison.csv\"\n    if os.path.exists(bcp):\n        bdf = pd.read_csv(bcp)\n        ms = [m for m in [\"nber_accuracy\",\"nber_f1\",\"nber_precision\",\"nber_recall\"] if m in bdf.columns]\n        if ms and \"method\" in bdf.columns:\n            fig, ax = plt.subplots(figsize=(10,6))\n            x = np.arange(len(bdf)); w = 0.8/len(ms)\n            for i, m in enumerate(ms):\n                ax.bar(x+i*w, bdf[m].astype(float), w, label=m.replace(\"nber_\",\"\").title(),\n                       color=[\"steelblue\",\"coral\",\"seagreen\",\"orchid\"][i%4], edgecolor=\"black\", lw=0.3)\n            ax.set_xticks(x+w*(len(ms)-1)/2); ax.set_xticklabels(bdf[\"method\"], rotation=15, ha=\"right\")\n            ax.set_ylabel(\"Score\"); ax.set_title(\"Baseline Comparison\"); ax.legend(); ax.set_ylim(0,1.05)\n            fig.savefig(f\"{FIGURES_DIR}/fig_baseline_comparison.png\", dpi=300, bbox_inches=\"tight\"); plt.close(fig); n_figs += 1\n\n    print(f\"  Generated {n_figs} inline figures\")\n\n# ======================================================================\n# STAGE 12: MULTI-SEED ERROR BARS (5 seeds)\n# ======================================================================\nimport numpy as np\nimport pandas as pd\n\nN_MAIN_SEEDS = 5\nEXTRA_SEEDS = [0, 1, 2, 3]\n\nMETRICS = [\n    'vamp2_score', 'spectral_gap', 'entropy_empirical', 'entropy_total',\n    'mean_irreversibility', 'detailed_balance_violation',\n    'fluctuation_theorem_ratio', 'n_complex_modes', 'complex_fraction',\n    'ktnd_nber_accuracy', 'ktnd_nber_f1',\n]\n\nmulti_seed_results = {}\n\nif skip(12, 'Multi-seed', [f'{RESULTS_DIR}/multi_seed_summary.json']):\n    results['multi_seed'] = True\n    if os.path.exists(f'{RESULTS_DIR}/multi_seed_summary.json'):\n        with open(f'{RESULTS_DIR}/multi_seed_summary.json') as f:\n            multi_seed_summary = json.load(f)\n    else:\n        multi_seed_summary = {}\nelse:\n    print(f\"\\n{'#'*70}\")\n    print(f\"#  MULTI-SEED ({N_MAIN_SEEDS} seeds: 42 + {EXTRA_SEEDS})\")\n    print(f\"{'#'*70}\")\n\n    for mode_tag in [\"univariate\", \"multiasset\"]:\n        ap = f\"{RESULTS_DIR}/analysis_results_{mode_tag}.json\"\n        if os.path.exists(ap):\n            with open(ap) as f:\n                multi_seed_results.setdefault(mode_tag, {})[42] = json.load(f)\n\n    for seed in EXTRA_SEEDS:\n        seed_dir = f\"{OUTPUT_DIR}/seed_{seed}\"\n        seed_results = f\"{seed_dir}/results\"\n        seed_models = f\"{seed_dir}/models\"\n        os.makedirs(seed_results, exist_ok=True)\n        os.makedirs(seed_models, exist_ok=True)\n\n        for mode_tag in [\"univariate\", \"multiasset\"]:\n            seed_ap = f\"{seed_results}/analysis_results_{mode_tag}.json\"\n            if os.path.exists(seed_ap):\n                print(f\"  Seed {seed} {mode_tag}: CACHED\")\n                with open(seed_ap) as f:\n                    multi_seed_results.setdefault(mode_tag, {})[seed] = json.load(f)\n                continue\n\n            print(f\"  Seed {seed} {mode_tag}: TRAINING...\", flush=True)\n            ok = run(f'Seed {seed} {mode_tag}',\n                f'{python} {REPO_DIR}/experiments/run_main.py'\n                f' --config config/default.yaml --mode {mode_tag} --seed {seed}'\n                f' --output-dir {seed_dir}')\n\n            if os.path.exists(seed_ap):\n                with open(seed_ap) as f:\n                    multi_seed_results.setdefault(mode_tag, {})[seed] = json.load(f)\n            else:\n                print(f\"  WARNING: No results for seed {seed} {mode_tag}\")\n\n    # Compute summaries\n    multi_seed_summary = {}\n    ms_summary_rows = []\n    for mode_tag in [\"univariate\", \"multiasset\"]:\n        if mode_tag not in multi_seed_results:\n            continue\n        seed_data = multi_seed_results[mode_tag]\n        seeds_present = sorted(seed_data.keys())\n        summary = {'n_seeds': len(seeds_present), 'seeds': seeds_present}\n        row = {'mode': mode_tag, 'n_seeds': len(seeds_present)}\n        for metric in METRICS:\n            vals = [float(seed_data[s][metric]) for s in seeds_present\n                    if seed_data[s].get(metric) is not None]\n            if vals:\n                mean_val, std_val = np.mean(vals), np.std(vals, ddof=1) if len(vals) > 1 else 0.0\n                summary[f'{metric}_mean'] = float(mean_val)\n                summary[f'{metric}_std'] = float(std_val)\n                row[f'{metric}_mean'] = float(mean_val)\n                row[f'{metric}_std'] = float(std_val)\n        multi_seed_summary[mode_tag] = summary\n        ms_summary_rows.append(row)\n\n    ms_json_path = f\"{RESULTS_DIR}/multi_seed_summary.json\"\n    with open(ms_json_path, 'w') as f:\n        json.dump(multi_seed_summary, f, indent=2, default=str)\n    if ms_summary_rows:\n        pd.DataFrame(ms_summary_rows).to_csv(f\"{RESULTS_DIR}/multi_seed_summary.csv\", index=False)\n\n    # Per-seed detailed CSV\n    ms_rows = []\n    for mode_tag in [\"univariate\", \"multiasset\"]:\n        if mode_tag not in multi_seed_results:\n            continue\n        for seed, data in sorted(multi_seed_results[mode_tag].items()):\n            row = {'mode': mode_tag, 'seed': seed}\n            for metric in METRICS:\n                row[metric] = data.get(metric)\n            ms_rows.append(row)\n    if ms_rows:\n        pd.DataFrame(ms_rows).to_csv(f\"{RESULTS_DIR}/multi_seed_detailed.csv\", index=False)\n\n    results['multi_seed'] = os.path.exists(f\"{RESULTS_DIR}/multi_seed_summary.json\")\n\n# ======================================================================\n# SAVE ALL RESULTS AS CSV\n# ======================================================================\nimport numpy as np\nimport pandas as pd\n\nprint(f\"\\n{'#'*70}\")\nprint(f\"#  SAVING ALL RESULTS AS CSV\")\nprint(f\"{'#'*70}\")\n\n# Pipeline summary\nsummary_rows = []\nfor mode_tag in [\"univariate\", \"multiasset\"]:\n    ap = f\"{RESULTS_DIR}/analysis_results_{mode_tag}.json\"\n    if not os.path.exists(ap):\n        continue\n    with open(ap) as f:\n        r = json.load(f)\n    row = {'mode': mode_tag, 'seed': 42}\n    for k in ['vamp2_score', 'spectral_gap', 'entropy_empirical', 'entropy_ci_lower',\n              'entropy_ci_upper', 'entropy_total', 'mean_irreversibility',\n              'detailed_balance_violation', 'fluctuation_theorem_ratio',\n              'n_complex_modes', 'complex_fraction', 'n_modes',\n              'ktnd_nber_accuracy', 'ktnd_nber_f1', 'ktnd_naive_accuracy',\n              'ktnd_mean_regime_duration', 'irrev_method']:\n        row[k] = r.get(k)\n    summary_rows.append(row)\nif summary_rows:\n    pd.DataFrame(summary_rows).to_csv(f\"{RESULTS_DIR}/pipeline_summary.csv\", index=False)\n    print(f\"  Saved: pipeline_summary.csv\")\n\n# Statistical tests as CSV\nfor mode_tag, suffix in [(\"univariate\", \"\"), (\"multiasset\", \"_multiasset\")]:\n    stp = f\"{RESULTS_DIR}/statistical_tests{suffix}.json\"\n    if not os.path.exists(stp):\n        continue\n    with open(stp) as f:\n        st = json.load(f)\n    stat_rows = []\n    for k, v in st.items():\n        if isinstance(v, dict):\n            row = {'test': k, 'mode': mode_tag}\n            for kk, vv in v.items():\n                if not isinstance(vv, (dict, list)):\n                    row[kk] = vv\n            stat_rows.append(row)\n    if stat_rows:\n        pd.DataFrame(stat_rows).to_csv(f\"{RESULTS_DIR}/statistical_tests{suffix}.csv\", index=False)\n        print(f\"  Saved: statistical_tests{suffix}.csv\")\n\n# CV as CSV\ncv_rows = []\nfor mode_tag in [\"univariate\", \"multiasset\"]:\n    cvp = f\"{RESULTS_DIR}/cv_results_{mode_tag}.json\"\n    if not os.path.exists(cvp):\n        continue\n    with open(cvp) as f:\n        cv = json.load(f)\n    row = {'mode': mode_tag, 'n_folds': cv.get('n_folds')}\n    for metric in ['vamp2', 'spectral_gap', 'db_violation', 'complex_fraction']:\n        row[f'{metric}_mean'] = cv.get(f'{metric}_mean')\n        row[f'{metric}_std'] = cv.get(f'{metric}_std')\n    cv_rows.append(row)\n    folds = cv.get('folds', cv.get('fold_results', []))\n    if folds:\n        pd.DataFrame(folds).to_csv(f\"{RESULTS_DIR}/cv_folds_{mode_tag}.csv\", index=False)\n        print(f\"  Saved: cv_folds_{mode_tag}.csv\")\nif cv_rows:\n    pd.DataFrame(cv_rows).to_csv(f\"{RESULTS_DIR}/cv_summary.csv\", index=False)\n    print(f\"  Saved: cv_summary.csv\")\n\n# Entropy calibration as CSV\necal_path = f\"{RESULTS_DIR}/entropy_calibration.json\"\nif os.path.exists(ecal_path):\n    with open(ecal_path) as f:\n        ecal = json.load(f)\n    points = ecal.get(\"calibration_points\", [])\n    if points:\n        pd.DataFrame(points).to_csv(f\"{RESULTS_DIR}/entropy_calibration.csv\", index=False)\n        print(f\"  Saved: entropy_calibration.csv\")\n\n# ======================================================================\n# FINAL REPORT\n# ======================================================================\nprint(f\"\\n{'='*70}\")\nprint(f\"  ALL OUTPUT FILES\")\nprint(f\"{'='*70}\")\nfor dirpath, dirnames, filenames in os.walk(OUTPUT_DIR):\n    for f in sorted(filenames):\n        fp = os.path.join(dirpath, f)\n        sz = os.path.getsize(fp)\n        rel = os.path.relpath(fp, OUTPUT_DIR)\n        print(f\"  {sz:>10,} bytes  {rel}\")\n\ntotal_min = (time.time() - pipeline_start) / 60\nn_ok = sum(v for v in results.values() if isinstance(v, bool) and v)\nn_total = len(results)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"  COMPLETE: {n_ok}/{n_total} stages ({total_min:.1f} min)\")\nprint(f\"{'='*70}\")\nfor name, ok in results.items():\n    print(f\"  {'OK' if ok else 'FAIL':6s}  {name}\")\n\nfor mode_tag, label in [(\"univariate\", \"Univariate (SPY)\"), (\"multiasset\", \"Multiasset (11 ETFs)\")]:\n    ap = f\"{RESULTS_DIR}/analysis_results_{mode_tag}.json\"\n    if not os.path.exists(ap):\n        continue\n    with open(ap) as f:\n        r = json.load(f)\n    print(f\"\\n  === {label} (seed 42) ===\")\n    print(f\"    VAMP-2:       {r.get('vamp2_score', 'N/A')}\")\n    print(f\"    Spectral gap: {r.get('spectral_gap', 'N/A')}\")\n    print(f\"    Entropy emp:  {r.get('entropy_empirical', 'N/A')} [{r.get('entropy_ci_lower','?')}, {r.get('entropy_ci_upper','?')}]\")\n    print(f\"    Entropy spec: {r.get('entropy_total', 'N/A')}\")\n    print(f\"    DB violation: {r.get('detailed_balance_violation', 'N/A')}\")\n    print(f\"    Complex:      {r.get('n_complex_modes', 'N/A')}/{r.get('n_modes', 'N/A')}\")\n    print(f\"    FT ratio:     {r.get('fluctuation_theorem_ratio', 'N/A')}\")\n    ktnd_acc = r.get('ktnd_nber_accuracy')\n    if ktnd_acc is not None:\n        print(f\"    NBER acc/F1:  {ktnd_acc:.3f} / {r.get('ktnd_nber_f1', 0):.3f}\")\n\nfor mode_tag in [\"univariate\", \"multiasset\"]:\n    cv_path = f\"{RESULTS_DIR}/cv_results_{mode_tag}.json\"\n    if os.path.exists(cv_path):\n        with open(cv_path) as f:\n            cv = json.load(f)\n        print(f\"\\n  === Walk-Forward CV ({mode_tag}, {cv.get('n_folds','?')} folds) ===\")\n        for metric in ['vamp2', 'spectral_gap', 'db_violation', 'complex_fraction']:\n            mk, sk = f'{metric}_mean', f'{metric}_std'\n            if mk in cv:\n                print(f\"    {metric:20s}  {cv[mk]:.4f} +/- {cv[sk]:.4f}\")\n\necal_path = f\"{RESULTS_DIR}/entropy_calibration.json\"\nif os.path.exists(ecal_path):\n    with open(ecal_path) as f:\n        ecal = json.load(f)\n    print(f\"\\n  === Entropy Calibration ===\")\n    for pt in ecal.get(\"calibration_points\", []):\n        if pt[\"ep_analytical\"] > 1e-6:\n            print(f\"    T2={pt['T2']:.1f}: analytical={pt['ep_analytical']:.4f}, KDE={pt['ep_kde']:.4f}, spectral={pt['ep_spectral']:.6f}\")\n\nfor mode_tag in [\"univariate\", \"multiasset\"]:\n    if mode_tag in multi_seed_summary:\n        s = multi_seed_summary[mode_tag]\n        print(f\"\\n  === Multi-Seed {mode_tag} ({s.get('n_seeds','?')} seeds) ===\")\n        for metric in METRICS:\n            mk = f'{metric}_mean'\n            if mk in s:\n                print(f\"    {metric:35s}  {s[mk]:.4f} +/- {s.get(f'{metric}_std',0):.4f}\")\n\nstat_path = f\"{RESULTS_DIR}/statistical_tests.json\"\nif os.path.exists(stat_path):\n    with open(stat_path) as f:\n        st = json.load(f)\n    print(f\"\\n  === Statistical Tests (IAAFT) ===\")\n    for k, v in st.items():\n        if isinstance(v, dict):\n            if v.get('skipped'):\n                print(f\"    {k}: SKIPPED\")\n            elif 'passed' in v:\n                print(f\"    {k}: {'PASSED' if v['passed'] else 'FAILED'}\")\n            elif 'p_value' in v:\n                d_str = f\", d={v['cohens_d']:.2f}\" if 'cohens_d' in v else \"\"\n                print(f\"    {k}: p={v['p_value']:.4f}{d_str}\")\n            elif 'any_significant' in v:\n                print(f\"    {k}: {'SIGNIFICANT' if v['any_significant'] else 'NOT SIGNIFICANT'}\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"  DONE ({total_min:.1f} min). All results in: {RESULTS_DIR}/\")\nprint(f\"  Next: Cell 3 (ablations) -> Cell 4 (figures) -> Cell 5 (download)\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "code",
   "source": "#@title 3. Ablations + Brownian gyrator (~6 hours, resume-safe)\n\nimport subprocess, time, json, os, sys\nimport numpy as np\nimport pandas as pd\n\nREPO_DIR = \"/content/ktnd_finance\"\nOUTPUT_DIR = \"/content/ktnd_finance/outputs\"\nRESULTS_DIR = \"/content/ktnd_finance/outputs/results\"\nFIGURES_DIR = \"/content/ktnd_finance/outputs/figures\"\nN_ABLATION_SEEDS = 10\n\nos.chdir(REPO_DIR)\npython = sys.executable\n\ndef run_streaming(name, cmd, check_files=None):\n    \"\"\"Run with live output streaming.\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"  STAGE: {name}\")\n    print(f\"{'='*70}\", flush=True)\n    t0 = time.time()\n    proc = subprocess.Popen(cmd, shell=True, cwd=REPO_DIR,\n        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n    for line in proc.stdout:\n        print(f\"  {line}\", end=\"\", flush=True)\n    proc.wait()\n    elapsed = time.time() - t0\n    if proc.returncode != 0:\n        print(f\"  >> {name}: FAILED (exit {proc.returncode}, {elapsed/60:.1f} min)\")\n        return False\n    if check_files:\n        missing = [f for f in check_files if not os.path.exists(f)]\n        if missing:\n            for f in missing:\n                print(f\"    MISSING: {f}\")\n            return False\n        for f in check_files:\n            print(f\"  OK: {os.path.basename(f)} ({os.path.getsize(f):,} bytes)\")\n    print(f\"  >> {name}: OK ({elapsed/60:.1f} min)\")\n    return True\n\ndef run(name, cmd, check_files=None):\n    \"\"\"Run a stage, print output, verify files.\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"  STAGE: {name}\")\n    print(f\"{'='*70}\")\n    t0 = time.time()\n    result = subprocess.run(cmd, shell=True, cwd=REPO_DIR, capture_output=True, text=True)\n    elapsed = time.time() - t0\n    if result.stdout:\n        for line in result.stdout.strip().split('\\n')[-30:]:\n            print(f\"  {line}\")\n    if result.returncode != 0:\n        if result.stderr:\n            for line in result.stderr.strip().split('\\n')[-20:]:\n                print(f\"  ! {line}\")\n        print(f\"  >> {name}: FAILED ({elapsed/60:.1f} min)\")\n        return False\n    if check_files:\n        missing = [f for f in check_files if not os.path.exists(f)]\n        if missing:\n            for f in missing:\n                print(f\"    MISSING: {f}\")\n            return False\n    print(f\"  >> {name}: OK ({elapsed/60:.1f} min)\")\n    return True\n\nt_start = time.time()\nresults = {}\n\n# ======================================================================\n# PART 1: ABLATION STUDY (10 seeds x ~32 variants)\n# ======================================================================\nresults['ablations'] = run_streaming(f'Ablations ({N_ABLATION_SEEDS} seeds)',\n    f'{python} -u experiments/run_ablations.py --config config/default.yaml'\n    f' --n-seeds {N_ABLATION_SEEDS} --n-jobs 1'\n    f' --output-dir {RESULTS_DIR}',\n    check_files=[f'{RESULTS_DIR}/ablation_summary.csv'])\n\nsummary_path = f\"{RESULTS_DIR}/ablation_summary.csv\"\nif os.path.exists(summary_path):\n    abl_df = pd.read_csv(summary_path)\n    print(f\"\\n  {len(abl_df)} ablation variants ({N_ABLATION_SEEDS} seeds each):\")\n    cols = ['name', 'n_valid', 'vamp2_mean', 'vamp2_std',\n            'spectral_gap_mean', 'spectral_gap_std',\n            'entropy_total_mean', 'entropy_total_std']\n    cols = [c for c in cols if c in abl_df.columns]\n    print(abl_df[cols].to_string(index=False))\n\n    if 'vamp2_mean' in abl_df.columns:\n        baseline = abl_df[abl_df['name'] == 'baseline']\n        if len(baseline) > 0:\n            bl_vamp2 = baseline['vamp2_mean'].values[0]\n            diff = abl_df.copy()\n            diff['vamp2_delta_pct'] = ((diff['vamp2_mean'] - bl_vamp2) / abs(bl_vamp2) * 100)\n            notable = diff[abs(diff['vamp2_delta_pct']) > 5].sort_values('vamp2_delta_pct')\n            if len(notable) > 0:\n                print(f\"\\n  Variants with >5% VAMP-2 change:\")\n                for _, row in notable.iterrows():\n                    print(f\"    {row['name']:40s}  {row['vamp2_delta_pct']:+.1f}%\")\n            # Save delta table as CSV\n            diff.to_csv(f\"{RESULTS_DIR}/ablation_delta.csv\", index=False)\n            print(f\"  Saved: ablation_delta.csv\")\n\n# ======================================================================\n# PART 2: BROWNIAN GYRATOR BENCHMARK\n# ======================================================================\nfrom scipy.linalg import solve_continuous_lyapunov\n\ndef analytical_ep(T1, T2, k=1.0, kappa=0.5):\n    A = np.array([[k, -kappa], [-kappa, k]])\n    D = np.array([[T1, 0.0], [0.0, T2]])\n    Sigma = solve_continuous_lyapunov(A, 2.0 * D)\n    Q = A - D @ np.linalg.inv(Sigma)\n    D_inv = np.diag([1.0/T1, 1.0/T2])\n    return np.trace(Q @ Sigma @ Q.T @ D_inv)\n\ngyrator_rows = []\nfor T2 in [1.0, 1.5, 2.0, 3.0, 5.0, 8.0]:\n    ep = analytical_ep(1.0, T2)\n    gyrator_rows.append({'T1': 1.0, 'T2': T2, 'ep_analytical': ep})\n    print(f\"  T1=1.0, T2={T2:.1f}: EP={ep:.6f}{'  (equilibrium)' if T2==1.0 else ''}\")\n\npd.DataFrame(gyrator_rows).to_csv(f\"{RESULTS_DIR}/gyrator_analytical.csv\", index=False)\nprint(f\"  Saved: gyrator_analytical.csv\")\n\nresults['gyrator'] = run('Brownian gyrator tests',\n    f'{python} -m pytest tests/test_synthetic.py::TestBrownianGyrator -v')\n\n# ======================================================================\n# SUMMARY\n# ======================================================================\ntotal_min = (time.time() - t_start) / 60\nprint(f\"\\n{'='*70}\")\nprint(f\"  ABLATIONS + GYRATOR COMPLETE ({total_min:.1f} min)\")\nprint(f\"{'='*70}\")\nfor name, ok in results.items():\n    print(f\"  {'OK' if ok else 'FAIL':6s}  {name}\")\nprint(f\"\\n  Next: Cell 4 (figures) -> Cell 5 (download)\")\nprint(f\"{'='*70}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. View figures (run after Cell 2 finishes)\n",
    "\n",
    "import glob, os\n",
    "from IPython.display import Image, display\n",
    "\n",
    "FIGURES_DIR = \"/content/ktnd_finance/outputs/figures\"\n",
    "\n",
    "pngs = sorted(glob.glob(f\"{FIGURES_DIR}/*.png\"))\n",
    "sup_dir = os.path.join(FIGURES_DIR, \"supplemental\")\n",
    "if os.path.exists(sup_dir):\n",
    "    pngs += sorted(glob.glob(f\"{sup_dir}/*.png\"))\n",
    "\n",
    "if pngs:\n",
    "    print(f\"Found {len(pngs)} figures:\\n\")\n",
    "    for p in pngs:\n",
    "        print(f\"--- {os.path.basename(p)} ---\")\n",
    "        display(Image(filename=p, width=800))\n",
    "        print()\n",
    "else:\n",
    "    print(\"No figures found. Make sure Cell 2 has finished running first.\")\n",
    "    print(f\"Checked: {FIGURES_DIR}\")\n",
    "    results_dir = \"/content/ktnd_finance/outputs/results\"\n",
    "    if os.path.exists(results_dir):\n",
    "        files = os.listdir(results_dir)\n",
    "        print(f\"Result files available ({len(files)}): {files}\")\n",
    "    else:\n",
    "        print(\"No results directory found - Cell 2 needs to run first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Download all results as zip\n",
    "\n",
    "!cd /content/ktnd_finance && zip -rq /content/ktnd_results.zip outputs/\n",
    "from google.colab import files\n",
    "files.download('/content/ktnd_results.zip')\n",
    "print(\"Download started.\")"
   ]
  }
 ]
}